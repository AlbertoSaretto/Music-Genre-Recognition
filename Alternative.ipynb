{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 17:19:51.390429: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "\n",
    "\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import utils\n",
    "import utils_mgr\n",
    "from utils_mgr import getAudio\n",
    "import warnings\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106574, 52)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load metadata and features.\n",
    "tracks = utils.load('Data/fma_metadata/tracks.csv')\n",
    "\n",
    "#Check tracks format\n",
    "tracks.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the desired subset among the entire dataset\n",
    "sub = 'small'\n",
    "raw_subset = tracks[tracks['set', 'subset'] <= sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of labels of interest for classification\n",
    "labels = raw_subset['track']['genre_top']\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of clean subset for the generation of training, test and validation sets\n",
    "\n",
    "meta_subset= utils_mgr.create_subset(raw_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_subset[:10]\n",
    "\n",
    "corrupted = [98565, 98567, 98569, 99134, 108925, 133297]\n",
    "\n",
    "#Remove corrupted songs\n",
    "for i in corrupted:\n",
    "    meta_subsest = meta_subset[meta_subset['index']!=i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split between taining, validation and test set according to original FMA split\n",
    "\n",
    "train_set = meta_subset[meta_subset[\"split\"] == \"training\"]\n",
    "val_set = meta_subset[meta_subset[\"split\"] == \"validation\"]\n",
    "test_set = meta_subset[meta_subset[\"split\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['labels'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 513)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tests with audio loading...\n",
    "\n",
    "\n",
    "audio, sr = getAudio(5)\n",
    "\n",
    "\n",
    "start = np.random.randint(0, (audio.shape[0]-2**17))\n",
    "audio = audio[start:start+2**17]\n",
    "# create lin-power mel spectrogram (discard last time bin)\n",
    "#S = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=4096, hop_length=1024)\n",
    "        # create log-power mel spectrogram\n",
    "#S = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "stft = np.abs(librosa.stft(audio, n_fft=2048, hop_length=1024))\n",
    "mel = librosa.feature.melspectrogram(sr=22050, S=stft**2, n_mels=513)\n",
    "mel = librosa.power_to_db(mel).T[:128]\n",
    "\n",
    "\n",
    "\n",
    "mel.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAudio(Dataset):\n",
    "\n",
    "    def __init__(self, df, transform = None, type = \"1D\"):\n",
    "        \n",
    "        # Get track index\n",
    "        self.track_ids = df['index'].values\n",
    "\n",
    "        #Get genre label\n",
    "        self.label = df['labels'].values\n",
    "\n",
    "        #Transform\n",
    "        self.transform = transform\n",
    "\n",
    "        #Select type of input\n",
    "        self.type = type\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.track_ids)\n",
    "\n",
    "\n",
    "    def create_input(self, i):\n",
    "\n",
    "        # Get audio\n",
    "\n",
    "        # load audio track\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            audio, sr = getAudio(self.track_ids[i])\n",
    "\n",
    "        #Select random clip from audio\n",
    "        start = np.random.randint(0, (audio.shape[0]-2**17))\n",
    "        audio = audio[start:start+2**17]\n",
    "        \n",
    "        if self.type ==  \"2D\":\n",
    "\n",
    "            #Get stft\n",
    "            stft = np.abs(librosa.stft(audio, n_fft=2048, hop_length=1024))\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                mel = librosa.feature.melspectrogram(sr=22050, S=stft**2, n_mels=513)[:,:128]\n",
    "                mel = librosa.power_to_db(mel).T\n",
    "            return mel\n",
    "        \n",
    "        return audio\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # get input and label\n",
    "\n",
    "        x = self.create_input(idx) \n",
    "        y = self.label[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x,y\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alberto/anaconda3/envs/mgr2/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transforms = v2.Compose([v2.ToTensor(),\n",
    "    v2.RandomResizedCrop(size=(128,513), antialias=True), \n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[1.0784853], std=[4.0071154]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MelDataset(test_set, transform = transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 128, 513])\n"
     ]
    }
   ],
   "source": [
    "for x,y in test_dataloader:\n",
    "    print(x.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.1\n"
     ]
    }
   ],
   "source": [
    "import librosa \n",
    "\n",
    "\n",
    "print(librosa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgr2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
