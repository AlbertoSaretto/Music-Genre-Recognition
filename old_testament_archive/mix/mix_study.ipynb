{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning:  CNN 1D + CNN 2D \n",
    "Goal: use already trained architectures to train a new classifier that gets information both from 1D and 2D data (raw audio + mel spectrogram).\n",
    "To do list:\n",
    "\n",
    "- Load the trained models and their weights\n",
    "- Set the model to evaluation mode and freeze their parameters in order not to track their gradients. We want to just use the models and train the classifier only\n",
    "- Build the new model:\n",
    "\n",
    "    - Extract CNN layers from NNET1D and NNET2D\n",
    "    - Create a new MixNet class that import these layers and adds a fully-connected block to their outputs.\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 15:18:47.496701: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from utils_mix import NNET1D, LN1D, NNET2, LN2D\n",
    "import numpy as np\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os \n",
    "from torch.optim import Adadelta\n",
    "import pytorch_lightning as pl\n",
    "import pickle\n",
    "import utils\n",
    "from utils_mgr import DataAudio, create_subset, MinMaxScaler\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained architectures \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network initialized\n",
      "Using default optimizer parameters\n",
      "Network initialized\n",
      "Using default optimizer parameters\n",
      "optimzier parameters: Adadelta (\n",
      "Parameter Group 0\n",
      "    differentiable: False\n",
      "    eps: 1e-06\n",
      "    foreach: None\n",
      "    lr: 1.0\n",
      "    maximize: False\n",
      "    rho: 0.9\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load model weights from checkpoint\n",
    "CKPT_PATH_1D = \"./1Dcheckpoint_Albi.ckpt\"\n",
    "CKPT_PATH_2D = \"../lightning_logs_2D_final/version_4/checkpoints/epoch=69-step=7000.ckpt\"\n",
    "nnet1d = LN1D.load_from_checkpoint(checkpoint_path=CKPT_PATH_1D).eval()\n",
    "nnet2d = LN2D.load_from_checkpoint(checkpoint_path=CKPT_PATH_2D).eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze the weights\n",
    "Setting `requires_grad = False` will stop Pytorch to tracking the gradients of the CNNs layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the weights\n",
    "for param in nnet1d.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in nnet2d.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset \n",
    "A new `Dataset` object is needed. This new class will import the audio file, select a random windows of $2^{18}$ samples and return:\n",
    "1. `audio`: window of raw audio. This will be the input of the 1D-CNN\n",
    "2. `mel`: mel spectrogram of the extracted window. This will be the input of the 2D-CNN\n",
    "3. `label`: one-hot encoded label to be predicted from the new classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import warnings\n",
    "import librosa\n",
    "from utils_mgr import getAudio\n",
    "\n",
    "class DataAudio_double(Dataset):\n",
    "\n",
    "    def __init__(self, df, transform = None, type = \"1D\"):\n",
    "        \n",
    "        # Get track index\n",
    "        self.track_ids = df['index'].values\n",
    "\n",
    "        #Get genre label\n",
    "        self.label = df['labels'].values\n",
    "\n",
    "        #Transform\n",
    "        self.transform = transform\n",
    "\n",
    "        #Select type of input\n",
    "        self.type = type\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.track_ids)\n",
    "\n",
    "\n",
    "    def create_input(self, i):\n",
    "      \n",
    "        # Get audio\n",
    "\n",
    "        # load audio track\n",
    "        #with warnings.catch_warnings():\n",
    "        #    warnings.simplefilter('ignore')\n",
    "\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            audio, sr = getAudio(self.track_ids[i])\n",
    "\n",
    "            #Select random clip from audio\n",
    "            start = np.random.randint(0, (audio.shape[0]-2**18))\n",
    "            audio = audio[start:start+2**18]\n",
    "            \n",
    "        \n",
    "            #Get 2D spectrogram\n",
    "            stft = np.abs(librosa.stft(audio, n_fft=4096, hop_length=2048))\n",
    "            \n",
    "            mel = librosa.feature.melspectrogram(sr=sr, S=stft**2, n_mels=513)[:,:128]\n",
    "            mel = librosa.power_to_db(mel, ref=np.max).T\n",
    "    \n",
    "        \n",
    "            return audio[np.newaxis,:], mel\n",
    "        \n",
    "            \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # get input and label\n",
    "        try:\n",
    "            audio,mel = self.create_input(idx)\n",
    "            y = self.label[idx] \n",
    "        except:\n",
    "            print(\"\\nNot able to load track number \", self.track_ids[idx], \" Loading next one\\n\")\n",
    "            audio,mel = self.create_input(idx+1)\n",
    "            y = self.label[idx]\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            mel = self.transform(mel)\n",
    "           \n",
    "        return audio,mel,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to load data, pass it through `DataAudio_double` class that applies transformations and create the `DataLoaders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# to visualize logs\n",
    "\n",
    "def import_and_preprocess_data(architecture_type=\"1D\"):\n",
    "\n",
    "    os.chdir(\"../.\")\n",
    "    files = os.listdir()\n",
    "    #print(files)\n",
    "    \"\"\"\n",
    "    This function uses metadata contained in tracks.csv to import mp3 files,\n",
    "    pass them through DataAudio class and eventually create Dataloaders.  \n",
    "    \n",
    "    \"\"\"\n",
    "    # Load metadata and features.\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "\n",
    "    #Select the desired subset among the entire dataset\n",
    "    sub = 'small'\n",
    "    raw_subset = tracks[tracks['set', 'subset'] <= sub] \n",
    "    \n",
    "    #Creation of clean subset for the generation of training, test and validation sets\n",
    "    meta_subset= create_subset(raw_subset)\n",
    "\n",
    "    # Remove corrupted files\n",
    "    corrupted = [98565, 98567, 98569, 99134, 108925, 133297]\n",
    "    meta_subset = meta_subset[~meta_subset['index'].isin(corrupted)]\n",
    "\n",
    "    #Split between taining, validation and test set according to original FMA split\n",
    "\n",
    "    train_set = meta_subset[meta_subset[\"split\"] == \"training\"]\n",
    "    val_set   = meta_subset[meta_subset[\"split\"] == \"validation\"]\n",
    "    test_set  = meta_subset[meta_subset[\"split\"] == \"test\"]\n",
    "\n",
    "    # Standard transformations for images\n",
    "\n",
    "    # There are two ways to normalize data: \n",
    "    #   1. Using  v2.Normalize(mean=[1.0784853], std=[4.0071154]). These values are computed with utils_mgr.mean_computer() function.\n",
    "    #   2. Using v2.Lambda and MinMaxScaler. This function is implemented in utils_mgr and resambles sklearn homonym function.\n",
    "\n",
    "    transforms = v2.Compose([v2.ToTensor(),\n",
    "        v2.RandomResizedCrop(size=(128,513), antialias=True), # Data Augmentation\n",
    "        v2.RandomHorizontalFlip(p=0.5), # Data Augmentation\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        #v2.Normalize(mean=[1.0784853], std=[4.0071154]),\n",
    "        v2.Lambda(lambda x: MinMaxScaler(x)) # see utils_mgr\n",
    "        ])\n",
    "\n",
    "    # Create the datasets and the dataloaders\n",
    "    \"\"\"  \n",
    "    train_dataset    = DataAudio_double(train_set, transform = transforms,type=architecture_type)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=os.cpu_count())\n",
    "    \"\"\"\n",
    "    val_dataset      = DataAudio_double(val_set, transform = transforms,type=architecture_type)\n",
    "    val_dataloader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=os.cpu_count())\n",
    "    \"\"\"\n",
    "    test_dataset     = DataAudio_double(test_set, transform = transforms,type=architecture_type)\n",
    "    test_dataloader  = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=os.cpu_count())\n",
    "    \"\"\"\n",
    "\n",
    "    #return train_dataloader, val_dataloader, test_dataloader\n",
    "    return val_dataloader, val_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/miniconda3/envs/fma/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/diego/miniconda3/envs/fma/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/diego/miniconda3/envs/fma/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = import_and_preprocess_data(architecture_type=\"2D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the shapes are what is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[0] torch.Size([64, 1, 262144])\n",
      "batch[1] torch.Size([64, 1, 128, 513])\n",
      "batch[2] torch.Size([64, 8])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(\"batch[0]\",batch[0].shape)\n",
    "    print(\"batch[1]\",batch[1].shape)\n",
    "    print(\"batch[2]\",batch[2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 256, kernel_size=(4, 513), stride=(1, 1))\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(256, 256, kernel_size=(4, 1), stride=(1, 1), padding=(2, 0))\n",
      "  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): Conv2d(256, 256, kernel_size=(4, 1), stride=(1, 1), padding=(1, 0))\n",
      "  (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Get all convolutional layers from nnet2d\n",
    "conv_layers = [layer for layer in nnet2d.modules() if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.ReLU)]\n",
    "\n",
    "# Build a new convolutional layer\n",
    "conv_block2D = nn.Sequential(*conv_layers[:9]) # [:9] to remove redundat ReLU layers taken by mistake from fc layers\n",
    "\n",
    "# Print the new convolutional layer\n",
    "print(conv_block2D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv1d(1, 16, kernel_size=(128,), stride=(32,), padding=(64,))\n",
      "  (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Conv1d(16, 32, kernel_size=(32,), stride=(2,), padding=(16,))\n",
      "  (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): Conv1d(32, 64, kernel_size=(16,), stride=(2,), padding=(8,))\n",
      "  (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): Conv1d(64, 128, kernel_size=(8,), stride=(2,), padding=(4,))\n",
      "  (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (11): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Get all convolutional layers from nnet1d\n",
    "conv_layers = [layer for layer in nnet1d.modules() if isinstance(layer, nn.Conv1d) or isinstance(layer, nn.BatchNorm1d) or isinstance(layer, nn.ReLU)]\n",
    "\n",
    "# Build a new convolutional layer\n",
    "conv_block1D = nn.Sequential(*conv_layers[:12])\n",
    "\n",
    "# Print the new convolutional layer\n",
    "print(conv_block1D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the weights are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0044, -0.0048, -0.0087,  ..., -0.0131, -0.0143, -0.0064],\n",
      "         [-0.0104, -0.0012, -0.0093,  ..., -0.0091, -0.0018, -0.0065],\n",
      "         [-0.0071, -0.0116, -0.0176,  ...,  0.0010, -0.0042,  0.0029],\n",
      "         [-0.0152, -0.0159, -0.0148,  ..., -0.0076, -0.0033, -0.0023]]])\n",
      "tensor([[[-0.0044, -0.0048, -0.0087,  ..., -0.0131, -0.0143, -0.0064],\n",
      "         [-0.0104, -0.0012, -0.0093,  ..., -0.0091, -0.0018, -0.0065],\n",
      "         [-0.0071, -0.0116, -0.0176,  ...,  0.0010, -0.0042,  0.0029],\n",
      "         [-0.0152, -0.0159, -0.0148,  ..., -0.0076, -0.0033, -0.0023]]])\n"
     ]
    }
   ],
   "source": [
    "# Print weights of the first conv layer in nnet1d\n",
    "for module in nnet2d.modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        print(module.weight[0])\n",
    "     \n",
    "        break\n",
    "\n",
    "# Print weights of the first conv layer in conv_block2D\n",
    "for module in conv_block2D.modules():\n",
    "    \n",
    "    if  isinstance(module, torch.nn.Conv2d):\n",
    "        print(module.weight[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the new model `MixNet`\n",
    "**For the real final architecture look at the .py files.** At the end there may be many less layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixNet(nn.Module):\n",
    "    def __init__(self, conv_block1D, conv_block2D):\n",
    "        super(MixNet, self).__init__()\n",
    "        self.conv_block1D = conv_block1D\n",
    "        self.conv_block2D = conv_block2D\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Add dropout layer\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512+2048, 512),\n",
    "            nn.ReLU(),\n",
    "            self.dropout, \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            self.dropout, \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            self.dropout,  \n",
    "            nn.Linear(128, 8),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        audio = x[0]\n",
    "        mel   = x[1]\n",
    "        \n",
    "        conv2d = self.conv_block2D(mel)\n",
    "        max_pool = F.max_pool2d(conv2d, kernel_size=(125,1))\n",
    "        avg_pool = F.avg_pool2d(conv2d, kernel_size=(125,1))\n",
    "        cat2d = torch.cat([max_pool,avg_pool],dim=1)\n",
    "        cat2d = cat2d.view(cat2d.size(0), -1) # cat2d shape torch.Size([1, 512])\n",
    "        \n",
    "        conv1d = self.conv_block1D(audio)\n",
    "        max_pool = F.max_pool1d(conv1d, kernel_size=125)\n",
    "        avg_pool = F.avg_pool1d(conv1d, kernel_size=125)\n",
    "        cat1d = torch.cat([max_pool,avg_pool],dim=1)\n",
    "        cat1d = cat1d.view(cat1d.size(0), -1) # cat1d dim = torch.Size([batch_size, 2048])\n",
    "\n",
    "        # Concatanate the two outputs and pass it to the classifier\n",
    "        # cat1d dim = torch.Size([batch_size, 2048])\n",
    "        # cat2d dim = torch.Size([batch_size, 512])\n",
    "        x = torch.cat([cat1d, cat2d], dim=1) \n",
    "        x = self.dropout(x)  # Add dropout layer\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = MixNet(conv_block1D, conv_block2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixNet(\n",
       "  (conv_block1D): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(128,), stride=(32,), padding=(64,))\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(16, 32, kernel_size=(32,), stride=(2,), padding=(16,))\n",
       "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv1d(32, 64, kernel_size=(16,), stride=(2,), padding=(8,))\n",
       "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv1d(64, 128, kernel_size=(8,), stride=(2,), padding=(4,))\n",
       "    (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_block2D): Sequential(\n",
       "    (0): Conv2d(1, 256, kernel_size=(4, 513), stride=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(256, 256, kernel_size=(4, 1), stride=(1, 1), padding=(2, 0))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(256, 256, kernel_size=(4, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2560, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): Linear(in_features=128, out_features=8, bias=True)\n",
       "    (10): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if everything is alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 262144])\n",
      "torch.Size([64, 8])\n",
      "tensor([[0.1200, 0.1088, 0.1244, 0.1430, 0.1169, 0.1292, 0.1228, 0.1349],\n",
      "        [0.1210, 0.1244, 0.1332, 0.1168, 0.1430, 0.1172, 0.1325, 0.1119],\n",
      "        [0.1187, 0.1250, 0.1277, 0.1204, 0.1334, 0.1245, 0.1264, 0.1239],\n",
      "        [0.1228, 0.1207, 0.1244, 0.1215, 0.1283, 0.1322, 0.1245, 0.1254],\n",
      "        [0.1279, 0.1271, 0.1230, 0.1276, 0.1220, 0.1245, 0.1353, 0.1126],\n",
      "        [0.1090, 0.1376, 0.1285, 0.1236, 0.1366, 0.1204, 0.1224, 0.1221],\n",
      "        [0.1315, 0.1065, 0.1329, 0.1279, 0.1306, 0.1244, 0.1341, 0.1121],\n",
      "        [0.1137, 0.1078, 0.1246, 0.1274, 0.1394, 0.1318, 0.1350, 0.1202],\n",
      "        [0.1119, 0.1250, 0.1199, 0.1252, 0.1361, 0.1226, 0.1271, 0.1321],\n",
      "        [0.1230, 0.1397, 0.1282, 0.1228, 0.1261, 0.1223, 0.1165, 0.1214],\n",
      "        [0.1121, 0.1386, 0.1148, 0.1277, 0.1317, 0.1302, 0.1227, 0.1223],\n",
      "        [0.0990, 0.1255, 0.1268, 0.1365, 0.1427, 0.1420, 0.1138, 0.1137],\n",
      "        [0.1188, 0.1246, 0.1287, 0.1283, 0.1333, 0.1191, 0.1322, 0.1150],\n",
      "        [0.1031, 0.1368, 0.1281, 0.1195, 0.1269, 0.1229, 0.1316, 0.1312],\n",
      "        [0.1169, 0.1235, 0.1358, 0.1248, 0.1274, 0.1352, 0.1145, 0.1219],\n",
      "        [0.1167, 0.1336, 0.1054, 0.1287, 0.1264, 0.1211, 0.1332, 0.1349],\n",
      "        [0.1258, 0.1185, 0.1218, 0.1220, 0.1234, 0.1247, 0.1378, 0.1261],\n",
      "        [0.1195, 0.1245, 0.1227, 0.1127, 0.1247, 0.1246, 0.1429, 0.1284],\n",
      "        [0.1100, 0.1397, 0.1150, 0.1226, 0.1190, 0.1382, 0.1176, 0.1379],\n",
      "        [0.1177, 0.1283, 0.1310, 0.1199, 0.1276, 0.1260, 0.1240, 0.1254],\n",
      "        [0.1147, 0.1390, 0.1206, 0.1284, 0.1310, 0.1234, 0.1189, 0.1240],\n",
      "        [0.1117, 0.1312, 0.1078, 0.1217, 0.1199, 0.1360, 0.1449, 0.1267],\n",
      "        [0.1137, 0.1343, 0.1221, 0.1331, 0.1267, 0.1090, 0.1338, 0.1274],\n",
      "        [0.1088, 0.1365, 0.1271, 0.1111, 0.1386, 0.1272, 0.1274, 0.1234],\n",
      "        [0.1188, 0.1234, 0.1158, 0.1180, 0.1326, 0.1270, 0.1369, 0.1275],\n",
      "        [0.1232, 0.1215, 0.1186, 0.1247, 0.1287, 0.1239, 0.1326, 0.1267],\n",
      "        [0.1133, 0.1191, 0.1251, 0.1272, 0.1293, 0.1303, 0.1297, 0.1261],\n",
      "        [0.1202, 0.1217, 0.1303, 0.1195, 0.1302, 0.1257, 0.1341, 0.1184],\n",
      "        [0.1044, 0.1282, 0.1137, 0.1271, 0.1416, 0.1346, 0.1343, 0.1159],\n",
      "        [0.1119, 0.1333, 0.1175, 0.1303, 0.1312, 0.1255, 0.1303, 0.1200],\n",
      "        [0.1110, 0.1283, 0.1187, 0.1282, 0.1245, 0.1359, 0.1314, 0.1220],\n",
      "        [0.1119, 0.1291, 0.1079, 0.1340, 0.1385, 0.1255, 0.1312, 0.1218],\n",
      "        [0.1017, 0.1355, 0.1423, 0.1306, 0.1186, 0.1185, 0.1395, 0.1133],\n",
      "        [0.1184, 0.1241, 0.1189, 0.1295, 0.1256, 0.1331, 0.1355, 0.1149],\n",
      "        [0.1239, 0.1200, 0.1226, 0.1221, 0.1358, 0.1270, 0.1300, 0.1186],\n",
      "        [0.1165, 0.1240, 0.1199, 0.1265, 0.1298, 0.1296, 0.1331, 0.1207],\n",
      "        [0.1117, 0.1309, 0.1268, 0.1249, 0.1262, 0.1253, 0.1205, 0.1337],\n",
      "        [0.1224, 0.1360, 0.1194, 0.1354, 0.1228, 0.1094, 0.1251, 0.1295],\n",
      "        [0.1098, 0.1265, 0.1199, 0.1159, 0.1329, 0.1249, 0.1373, 0.1328],\n",
      "        [0.1057, 0.1200, 0.1220, 0.1210, 0.1318, 0.1305, 0.1358, 0.1333],\n",
      "        [0.1145, 0.1342, 0.1202, 0.1334, 0.1213, 0.1199, 0.1320, 0.1245],\n",
      "        [0.1112, 0.1209, 0.1307, 0.1249, 0.1299, 0.1361, 0.1210, 0.1253],\n",
      "        [0.1255, 0.1302, 0.1292, 0.1255, 0.1262, 0.1313, 0.1226, 0.1094],\n",
      "        [0.1219, 0.1263, 0.1354, 0.1308, 0.1269, 0.1186, 0.1324, 0.1076],\n",
      "        [0.1312, 0.1276, 0.1265, 0.1193, 0.1204, 0.1312, 0.1234, 0.1203],\n",
      "        [0.1386, 0.1186, 0.1202, 0.1199, 0.1346, 0.1109, 0.1340, 0.1231],\n",
      "        [0.1301, 0.1174, 0.1294, 0.1257, 0.1298, 0.1326, 0.1232, 0.1117],\n",
      "        [0.1120, 0.1189, 0.1210, 0.1245, 0.1307, 0.1388, 0.1141, 0.1400],\n",
      "        [0.1134, 0.1403, 0.1274, 0.1282, 0.1334, 0.1177, 0.1161, 0.1236],\n",
      "        [0.1127, 0.1229, 0.1175, 0.1366, 0.1300, 0.1288, 0.1253, 0.1260],\n",
      "        [0.1134, 0.1327, 0.1233, 0.1312, 0.1381, 0.1292, 0.1214, 0.1106],\n",
      "        [0.1063, 0.1420, 0.1179, 0.1275, 0.1250, 0.1324, 0.1311, 0.1178],\n",
      "        [0.1245, 0.1271, 0.1276, 0.1230, 0.1186, 0.1341, 0.1275, 0.1176],\n",
      "        [0.1175, 0.1196, 0.1204, 0.1294, 0.1398, 0.1169, 0.1362, 0.1202],\n",
      "        [0.0984, 0.1443, 0.1167, 0.1319, 0.1274, 0.1314, 0.1249, 0.1251],\n",
      "        [0.1122, 0.1444, 0.1316, 0.1177, 0.1214, 0.1172, 0.1393, 0.1162],\n",
      "        [0.1159, 0.1309, 0.1287, 0.1364, 0.1180, 0.1237, 0.1206, 0.1258],\n",
      "        [0.1197, 0.1258, 0.1190, 0.1261, 0.1286, 0.1239, 0.1341, 0.1228],\n",
      "        [0.1109, 0.1390, 0.1259, 0.1325, 0.1295, 0.1174, 0.1197, 0.1251],\n",
      "        [0.1116, 0.1340, 0.1317, 0.1209, 0.1298, 0.1264, 0.1348, 0.1108],\n",
      "        [0.1135, 0.1399, 0.1221, 0.1297, 0.1274, 0.1242, 0.1333, 0.1098],\n",
      "        [0.1203, 0.1153, 0.1122, 0.1341, 0.1328, 0.1276, 0.1345, 0.1233],\n",
      "        [0.1157, 0.1238, 0.1321, 0.1229, 0.1213, 0.1315, 0.1330, 0.1197],\n",
      "        [0.1194, 0.1255, 0.1264, 0.1141, 0.1328, 0.1284, 0.1365, 0.1169]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    input = batch[:2]\n",
    "    print(input[0].shape) # Audio shape: torch.Size([64, 1, 262144])\n",
    "    label = batch[2]\n",
    "    print(label.shape) # Label shape: torch.Size([64, 8])\n",
    "    output = mn(input)\n",
    "    print(output) \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only classifier layers need grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_block1D.0.weight: False\n",
      "conv_block1D.0.bias: False\n",
      "conv_block1D.1.weight: False\n",
      "conv_block1D.1.bias: False\n",
      "conv_block1D.3.weight: False\n",
      "conv_block1D.3.bias: False\n",
      "conv_block1D.4.weight: False\n",
      "conv_block1D.4.bias: False\n",
      "conv_block1D.6.weight: False\n",
      "conv_block1D.6.bias: False\n",
      "conv_block1D.7.weight: False\n",
      "conv_block1D.7.bias: False\n",
      "conv_block1D.9.weight: False\n",
      "conv_block1D.9.bias: False\n",
      "conv_block1D.10.weight: False\n",
      "conv_block1D.10.bias: False\n",
      "conv_block2D.0.weight: False\n",
      "conv_block2D.0.bias: False\n",
      "conv_block2D.1.weight: False\n",
      "conv_block2D.1.bias: False\n",
      "conv_block2D.3.weight: False\n",
      "conv_block2D.3.bias: False\n",
      "conv_block2D.4.weight: False\n",
      "conv_block2D.4.bias: False\n",
      "conv_block2D.6.weight: False\n",
      "conv_block2D.6.bias: False\n",
      "conv_block2D.7.weight: False\n",
      "conv_block2D.7.bias: False\n",
      "classifier.0.weight: True\n",
      "classifier.0.bias: True\n",
      "classifier.3.weight: True\n",
      "classifier.3.bias: True\n",
      "classifier.6.weight: True\n",
      "classifier.6.bias: True\n",
      "classifier.9.weight: True\n",
      "classifier.9.bias: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in mn.named_parameters():\n",
    "    print(f'{name}: {param.requires_grad}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
