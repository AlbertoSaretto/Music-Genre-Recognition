{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import os \n",
    "from torch.optim import Adadelta\n",
    "\n",
    "\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "\n",
    "import utils\n",
    "import utils_mgr\n",
    "from utils_mgr import getAudio, DataAudio\n",
    "\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_and_preprocess_data(archtecture_type = \"1D\"):\n",
    "    # Load metadata and features.\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "\n",
    "\n",
    "    #Select the desired subset among the entire dataset\n",
    "    sub = 'small'\n",
    "    raw_subset = tracks[tracks['set', 'subset'] <= sub] \n",
    "    \n",
    "    #Creation of clean subset for the generation of training, test and validation sets\n",
    "    meta_subset= utils_mgr.create_subset(raw_subset)\n",
    "\n",
    "    # Remove corrupted files\n",
    "    corrupted = [98565, 98567, 98569, 99134, 108925, 133297]\n",
    "    meta_subset = meta_subset[~meta_subset['index'].isin(corrupted)]\n",
    "\n",
    "    #Split between taining, validation and test set according to original FMA split\n",
    "\n",
    "    train_set = meta_subset[meta_subset[\"split\"] == \"training\"]\n",
    "    val_set   = meta_subset[meta_subset[\"split\"] == \"validation\"]\n",
    "    test_set  = meta_subset[meta_subset[\"split\"] == \"test\"]\n",
    "\n",
    "    # Standard transformations for images\n",
    "    # Mean and std are computed on one file of the training set\n",
    "    transforms = v2.Compose([torch.Tensor,\n",
    "                            lambda x: x/0.21469156])  #To normalize data\n",
    "\n",
    "    # Create the datasets and the dataloaders\n",
    "    \"\"\"\n",
    "    train_dataset    = DataAudio(train_set, transform = transforms, type=archtecture_type)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8)\n",
    "\n",
    "    val_dataset      = DataAudio(val_set, transform = transforms, type=archtecture_type)\n",
    "    val_dataloader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=8)\n",
    "\n",
    "    \"\"\"\n",
    "    test_dataset     = DataAudio(test_set, transform = transforms, type=archtecture_type)\n",
    "    test_dataloader  = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=8)\n",
    "\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
