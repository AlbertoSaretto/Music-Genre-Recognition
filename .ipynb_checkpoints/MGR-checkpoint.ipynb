{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce0fa1d",
   "metadata": {},
   "source": [
    "# CNN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.chdir(\"/drive/MyDrive/data\")\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, RandomAffine, RandomHorizontalFlip, RandomVerticalFlip, ColorJitter\n",
    "import torch\n",
    "\n",
    "import random\n",
    "import utils\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c26c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveheavy(df, name, n):  #functions to save havy dataframes in multiple files\n",
    "    l= len(df)\n",
    "    for i in range(n-1):\n",
    "        df.iloc[int((l/n)*i):int((l/n)*(i+1))].to_hdf(f'{name}_{i+1}.h5', 'x', mode='w')\n",
    "    df.iloc[int((l/n)*(n-1)):l].to_hdf(f'{name}_{n}.h5', 'x', mode='w')\n",
    "    \n",
    "def readheavy(name, n, column):\n",
    "    result = pd.DataFrame(columns=column)\n",
    "    for i in range(n):\n",
    "        df = pd.read_hdf(f'Data/MelMfcc/{name}_{i+1}.h5', 'x')\n",
    "        print(i)\n",
    "        result = pd.concat([result, df], ignore_index=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddfbfa80",
   "metadata": {},
   "outputs": [
    {
     "ename": "HDF5ExtError",
     "evalue": "HDF5 error back trace\n\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5Dio.c\", line 199, in H5Dread\n    can't read data\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5Dio.c\", line 603, in H5D__read\n    can't read data\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5Dchunk.c\", line 2316, in H5D__chunk_read\n    chunked read failed\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5Dscatgath.c\", line 550, in H5D__scatgath_read\n    datatype conversion failed\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5T.c\", line 5025, in H5T_convert\n    datatype conversion failed\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5Tconv.c\", line 3221, in H5T__conv_vlen\n    memory allocation failed for type conversion\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5FL.c\", line 922, in H5FL_blk_malloc\n    memory allocation failed for chunk\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5FL.c\", line 250, in H5FL_malloc\n    memory allocation failed for chunk\n\nEnd of HDF5 error back trace\n\nVLArray._read_array: Problems reading the array data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHDF5ExtError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Restore Datasets\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData/MelMfcc/test.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m validation \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_hdf(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/MelMfcc/validation.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m training \u001b[38;5;241m=\u001b[39m readheavy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m8\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmfcc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py:442\u001b[0m, in \u001b[0;36mread_hdf\u001b[1;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    438\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey must be provided when HDF5 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile contains multiple datasets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m                 )\n\u001b[0;32m    441\u001b[0m         key \u001b[38;5;241m=\u001b[39m candidate_only_group\u001b[38;5;241m.\u001b[39m_v_pathname\n\u001b[1;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43miterator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_close\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m):\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, HDFStore):\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;66;03m# if there is an error, close the store if we opened it.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py:872\u001b[0m, in \u001b[0;36mHDFStore.select\u001b[1;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;66;03m# create the iterator\u001b[39;00m\n\u001b[0;32m    859\u001b[0m it \u001b[38;5;241m=\u001b[39m TableIterator(\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    861\u001b[0m     s,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    869\u001b[0m     auto_close\u001b[38;5;241m=\u001b[39mauto_close,\n\u001b[0;32m    870\u001b[0m )\n\u001b[1;32m--> 872\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py:1947\u001b[0m, in \u001b[0;36mTableIterator.get_result\u001b[1;34m(self, coordinates)\u001b[0m\n\u001b[0;32m   1944\u001b[0m     where \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhere\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# directly return the result\u001b[39;00m\n\u001b[1;32m-> 1947\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py:856\u001b[0m, in \u001b[0;36mHDFStore.select.<locals>.func\u001b[1;34m(_start, _stop, _where)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(_start, _stop, _where):\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py:3220\u001b[0m, in \u001b[0;36mBlockManagerFixed.read\u001b[1;34m(self, where, columns, start, stop)\u001b[0m\n\u001b[0;32m   3217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnblocks):\n\u001b[0;32m   3219\u001b[0m     blk_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_index(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_items\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 3220\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblock\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3222\u001b[0m     columns \u001b[38;5;241m=\u001b[39m items[items\u001b[38;5;241m.\u001b[39mget_indexer(blk_items)]\n\u001b[0;32m   3223\u001b[0m     df \u001b[38;5;241m=\u001b[39m DataFrame(values\u001b[38;5;241m.\u001b[39mT, columns\u001b[38;5;241m=\u001b[39mcolumns, index\u001b[38;5;241m=\u001b[39maxes[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py:2884\u001b[0m, in \u001b[0;36mGenericFixed.read_array\u001b[1;34m(self, key, start, stop)\u001b[0m\n\u001b[0;32m   2881\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(attrs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransposed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, tables\u001b[38;5;241m.\u001b[39mVLArray):\n\u001b[1;32m-> 2884\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[start:stop]\n\u001b[0;32m   2885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2886\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m _ensure_decoded(\u001b[38;5;28mgetattr\u001b[39m(attrs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tables\\vlarray.py:662\u001b[0m, in \u001b[0;36mVLArray.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    660\u001b[0m         key \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows\n\u001b[0;32m    661\u001b[0m     (start, stop, step) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_range(key, key \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m    664\u001b[0m     start, stop, step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_range(\n\u001b[0;32m    665\u001b[0m         key\u001b[38;5;241m.\u001b[39mstart, key\u001b[38;5;241m.\u001b[39mstop, key\u001b[38;5;241m.\u001b[39mstep)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tables\\vlarray.py:802\u001b[0m, in \u001b[0;36mVLArray.read\u001b[1;34m(self, start, stop, step)\u001b[0m\n\u001b[0;32m    800\u001b[0m     listarr \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 802\u001b[0m     listarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m atom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matom\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(atom, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# it is a pseudo-atom\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tables\\hdf5extension.pyx:2133\u001b[0m, in \u001b[0;36mtables.hdf5extension.VLArray._read_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mHDF5ExtError\u001b[0m: HDF5 error back trace\n\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5Dio.c\", line 199, in H5Dread\n    can't read data\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5Dio.c\", line 603, in H5D__read\n    can't read data\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5Dchunk.c\", line 2316, in H5D__chunk_read\n    chunked read failed\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5Dscatgath.c\", line 550, in H5D__scatgath_read\n    datatype conversion failed\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5T.c\", line 5025, in H5T_convert\n    datatype conversion failed\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5Tconv.c\", line 3221, in H5T__conv_vlen\n    memory allocation failed for type conversion\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5FL.c\", line 922, in H5FL_blk_malloc\n    memory allocation failed for chunk\n  File \"C:\\ci\\hdf5_1655187604574\\work\\src\\H5FL.c\", line 250, in H5FL_malloc\n    memory allocation failed for chunk\n\nEnd of HDF5 error back trace\n\nVLArray._read_array: Problems reading the array data."
     ]
    }
   ],
   "source": [
    "#Restore Datasets\n",
    "\n",
    "test = pd.read_hdf('Data/MelMfcc/test.h5', 'x')\n",
    "validation = pd.read_hdf('Data/MelMfcc/validation.h5', 'x')\n",
    "training = readheavy('training', 8, ['mel', 'mfcc', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0454bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#Class for the creation of torch manageble datasets, with Format one can select the desired input column \n",
    "class DataAudio(Dataset):\n",
    "\n",
    "    def __init__(self, split, Format, transform=None):\n",
    "        self.x = split[Format]\n",
    "        self.y = split['y']\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(32000, len(self.x))\n",
    "        #return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx, 0, 0].astype(float)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6997f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(), #this converts numpy or Pil image to torch tensor and normalizes it in 0, 1\n",
    "    RandomAffine((0.05, 0.05)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76641069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of torch suited dataset classes  (change string 'mel' to select desired Format)\n",
    "training_dataset = DataAudio(training, 'mel',transforms)\n",
    "validation_dataset = DataAudio(validation, 'mel',transforms)\n",
    "test_dataset = DataAudio(test, 'mel',transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of dataloader classes\n",
    "batch_size = 64\n",
    "training_dataloader = DataLoader(training_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
