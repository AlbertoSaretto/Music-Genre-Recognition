{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce0fa1d",
   "metadata": {},
   "source": [
    "# CNN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef3ceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alber\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "C:\\Users\\alber\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] Impossibile trovare la procedura specificata'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#os.chdir(\"/drive/MyDrive/data\")\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, RandomAffine, RandomHorizontalFlip, RandomVerticalFlip, ColorJitter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "import random\n",
    "import utils\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)\n",
    "\n",
    "sr = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "130d6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c26c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveheavy(a, name, n):  #functions to save havy dataframes in multiple files\n",
    "    l= len(a)\n",
    "    if(n>1):\n",
    "        for i in range(n-1):\n",
    "            a_i = a[int((l/n)*i):int((l/n)*(i+1))]\n",
    "            np.save(f'{name}_{i+1}.npy', a_i)\n",
    "    a_i = a[int((l/n)*(n-1)):l]\n",
    "    np.save(f'{name}_{n}.npy', a_i)\n",
    "    \n",
    "def readheavy(name, n, Dir):\n",
    "    a = np.array([0,0])\n",
    "    for i in range(n):\n",
    "        new_a = np.load(f'Data/{Dir}/{name}_{i+1}.npy', allow_pickle = True)\n",
    "        a = np.vstack([a, new_a])\n",
    "    return a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddfbfa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restore Datasets\n",
    "\n",
    "test = readheavy('test', 2, 'Audio')\n",
    "training = readheavy('training', 16, 'Audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba3705e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6394, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8158f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stft(a):\n",
    "    for j in range(len(a)):\n",
    "        audio = a[j, 0]\n",
    "        stft = np.abs(librosa.stft(audio, n_fft=1024, hop_length=512))\n",
    "        a[j ,0] = stft\n",
    "    return a\n",
    "\n",
    "def get_mel(a):\n",
    "    for j in range(len(a)):\n",
    "        audio = a[j,0]\n",
    "        stft = np.abs(librosa.stft(audio, n_fft=1024, hop_length=512))\n",
    "        mel = mel = librosa.feature.melspectrogram(sr=sr, S=stft**2)\n",
    "        a[j,0] = mel\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75a2a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I convert Audio into stft data\n",
    "\n",
    "test = get_stft(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb02f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = get_stft(validation)\n",
    "training = get_stft(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdff50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_stft(a, n_samples):\n",
    "    a_clip = np.array([0,0])\n",
    "    for j in range(len(a)):\n",
    "        full = a[j, 0].T\n",
    "        n=0\n",
    "        while (n<(len(full)-n_samples)):\n",
    "            clip = full[n: (n+n_samples)]\n",
    "            y = a[j, 1]\n",
    "            new_row = np.array([clip, new_row])\n",
    "            a_clip = np.vstack([a_clip, new_row)\n",
    "            n+=int(n_samples/2)\n",
    "    return a_clip[1:]\n",
    "\n",
    "def clip_audio(df, n_samples):\n",
    "    a_clip = np.array([0,0])\n",
    "    for j in range(len(a)):\n",
    "        full = a[j, 0]\n",
    "        n=0\n",
    "        while (n<(len(full)-n_samples)):\n",
    "            clip = full[n: (n+n_samples)]\n",
    "            y = a[j, 1]\n",
    "            new_row = np.array([clip, new_row])\n",
    "            a_clip = np.vstack([a_clip, new_row)\n",
    "            n+=int(n_samples/2)\n",
    "    return a_clip[1:]\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5734007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clip = clip_stft(test, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d0f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_clip = clip_stft(validation, 128)\n",
    "training_clip = clip_stft(training, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "912134f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 513)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clip['stft'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0454bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#Class for the creation of torch manageble datasets, with Format one can select the desired input column \n",
    "class DataAudio(Dataset):\n",
    "\n",
    "    def __init__(self, split, transform=None):\n",
    "        self.x = split[0]\n",
    "        self.y = split[1]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        #return min(32000, len(self.x))\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx, 0, 0].astype(float)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "081acbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 513)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clip['stft'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c6997f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(), #this converts numpy or Pil image to torch tensor and normalizes it in 0, 1\n",
    "    #RandomAffine((0.05, 0.05)),\n",
    "    #RandomHorizontalFlip(),      #Per ruotare immagini\n",
    "    #RandomVerticalFlip()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76641069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of torch suited dataset classes  (change string 'mel' to select desired Format)\n",
    "test_dataset = DataAudio(test_clip, 'stft',transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = DataAudio(validation_clip, 'stft',transforms)\n",
    "training_dataset = DataAudio(training_clip, 'stft',transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad5d3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of dataloader classes\n",
    "batch_size = 10\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = DataLoader(validation_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())\n",
    "training_dataloader = DataLoader(training_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e00f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to add dropout layers \n",
    "\n",
    "class NNET1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNET1, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=128,kernel_size=(4,513)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(4,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(4,1)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Input of fc1 is 256\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 150),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(150, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        max_pool = F.max_pool2d(x, kernel_size=(26,1))\n",
    "        avg_pool = F.avg_pool2d(x, kernel_size=(26,1))\n",
    "        x = max_pool + avg_pool\n",
    "        x = self.fc(x.view(-1, 256))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dcbe510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNET2(nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(NNET2, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.c1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=256,kernel_size=(4,513)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.c2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(4, 1),padding=(2,0)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.c3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(4, 1),padding=(1,0)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 150),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(150, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        c1 = self.c1(x)\n",
    "        c2 = self.c2(c1)\n",
    "        c3 = self.c3(c2)\n",
    "        x = c1 + c3\n",
    "        max_pool = F.max_pool2d(x, kernel_size=(125,1))\n",
    "        avg_pool = F.avg_pool2d(x, kernel_size=(125,1))\n",
    "        x = max_pool + avg_pool\n",
    "        x = self.fc(x.view(-1, 256))\n",
    "        return x \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64e98891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Fake spectrogram 128x513\n",
    "x = torch.randn(1, 1, 128, 513)\n",
    "\n",
    "# Create model\n",
    "model = NNET2()\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "# Print output shape\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6510d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9c694ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [[0.00021080021, 0.00020972929, 0.00020654705,...\n",
       "1     [[2.1542284, 3.2158666, 22.255291, 23.587381, ...\n",
       "2     [[1.5419899, 8.78172, 25.200851, 50.310005, 74...\n",
       "3     [[2.275202, 2.9461339, 5.1720843, 12.862928, 1...\n",
       "4     [[2.190529, 3.119165, 4.2509694, 18.54553, 37....\n",
       "5     [[0.554729, 13.509325, 27.020035, 20.695541, 1...\n",
       "6     [[2.057228, 4.084349, 5.652993, 8.063932, 8.70...\n",
       "7     [[1.0614991, 4.8038635, 10.438138, 32.33236, 3...\n",
       "8     [[0.9206511, 9.127373, 29.346125, 32.353477, 1...\n",
       "9     [[2.5374625, 6.8586917, 44.346695, 50.825443, ...\n",
       "10    [[0.24397269, 1.8173068, 12.106457, 21.054564,...\n",
       "11    [[1.2853845, 21.224907, 61.50625, 53.425743, 2...\n",
       "12    [[0.34479684, 3.5025811, 13.639755, 16.223742,...\n",
       "13    [[0.22271799, 5.5677457, 3.4634311, 21.67279, ...\n",
       "14    [[0.9326537, 9.896021, 32.207104, 28.57739, 32...\n",
       "15    [[0.39727607, 16.838356, 37.255775, 50.28999, ...\n",
       "16    [[1.1973219, 10.425424, 31.496302, 33.228195, ...\n",
       "17    [[0.080669165, 21.352627, 68.46717, 56.73909, ...\n",
       "18    [[0.25641736, 1.4988838, 2.6711216, 6.1885276,...\n",
       "19    [[0.0011398592, 0.0011328997, 0.0011122471, 0....\n",
       "20    [[1.3624768, 25.957638, 55.78992, 49.723186, 5...\n",
       "21    [[1.3190223, 2.813881, 5.045543, 5.8230085, 4....\n",
       "22    [[2.1234665, 23.506088, 25.772575, 27.813833, ...\n",
       "23    [[0.046210393, 0.30973047, 0.58769053, 1.21368...\n",
       "24    [[0.27850223, 16.387001, 83.97521, 109.413345,...\n",
       "25    [[0.30973664, 3.1621358, 2.815609, 7.7984896, ...\n",
       "26    [[4.4100494, 10.086588, 22.267933, 75.119125, ...\n",
       "27    [[0.0077318707, 0.068607084, 0.29747304, 0.635...\n",
       "28    [[4.7402143, 5.9161215, 10.153203, 14.732593, ...\n",
       "29    [[0.2781582, 0.77704996, 1.6492354, 2.0718834,...\n",
       "30    [[0.18089947, 0.6630199, 0.6448417, 1.064791, ...\n",
       "31    [[0.63378525, 1.3355905, 16.006405, 64.00869, ...\n",
       "32    [[0.17648041, 0.68999636, 0.77365273, 8.013492...\n",
       "33    [[0.21647076, 0.10941873, 1.4866881, 1.7701486...\n",
       "34    [[0.98810923, 24.758072, 72.01628, 67.41353, 2...\n",
       "35    [[1.2073286, 2.7057133, 2.744528, 20.648012, 4...\n",
       "36    [[0.08206847, 0.7128847, 1.722993, 1.8746586, ...\n",
       "37    [[1.903303, 3.5146434, 2.5106132, 11.951701, 5...\n",
       "Name: stft, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.dataset.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f841c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(test_dataloader))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam, Adadelta\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "#iterator = tqdm(test_dataloader)\n",
    "for batch_x, batch_y in test_dataloader:\n",
    "#    batch_x = batch_x.to(device)\n",
    "#    batch_y = batch_y.to(device)\n",
    "    print(batch_x)\n",
    "    print(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa893e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/190 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam, Adadelta\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "opt = Adadelta(model.parameters(), lr=1e-2, weight_decay = 0)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "epochs=2\n",
    "best_val = np.inf\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    iterator = tqdm(test_dataloader)\n",
    "    for batch_x, batch_y in iterator:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        y_pred = model(batch_x)\n",
    "\n",
    "        loss = loss_fn(y_pred, batch_y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        iterator.set_description(f\"Train loss: {loss.detach().cpu().numpy()}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        true = []\n",
    "        for batch_x, batch_y in tqdm(validation_dataloader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            y_pred = model(batch_x)\n",
    "\n",
    "            predictions.append(y_pred)\n",
    "            true.append(batch_y)\n",
    "        predictions = torch.cat(predictions, axis=0)\n",
    "        true = torch.cat(true, axis=0)\n",
    "        val_loss = loss_fn(predictions, true)\n",
    "        val_acc = (torch.sigmoid(predictions).round() == true).float().mean()\n",
    "        print(f\"loss: {val_loss}, accuracy: {val_acc}\")\n",
    "    \n",
    "    if val_loss < best_val:\n",
    "        print(\"Saved Model\")\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        best_val = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee99d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_curve, roc_auc_score\n",
    "\n",
    "def evaluate_network(dataloader, model, data_split):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        true = []\n",
    "        for batch_x, batch_y in tqdm(dataloader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            y_pred = model(batch_x)\n",
    "\n",
    "            predictions.append(y_pred)\n",
    "            true.append(batch_y)\n",
    "        predictions = torch.cat(predictions, axis=0)\n",
    "        true = torch.cat(true, axis=0)\n",
    "        loss = loss_fn(predictions, true).detach().cpu().numpy()\n",
    "        predictions = torch.sigmoid(predictions).detach().cpu().numpy()\n",
    "        true = true.detach().cpu().numpy()\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(true, predictions)\n",
    "        auc = roc_auc_score(true, predictions)\n",
    "        predictions = predictions.round()\n",
    "        precision, recall, fscore, _= precision_recall_fscore_support(true, predictions, average='binary')\n",
    "        accuracy = accuracy_score(true, predictions)\n",
    "\n",
    "        print(f\"{data_split} loss: {loss}, accuracy: {accuracy}, precision: {precision}, recall: {recall}, f1: {fscore}, roc_auc: {auc}\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'{data_split} receiver operating characteristic (ROC)')\n",
    "        plt.legend(loc=\"lower right\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
