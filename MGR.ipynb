{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce0fa1d",
   "metadata": {},
   "source": [
    "# CNN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef3ceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alber\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "C:\\Users\\alber\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] Impossibile trovare la procedura specificata'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#os.chdir(\"/drive/MyDrive/data\")\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, RandomAffine, RandomHorizontalFlip, RandomVerticalFlip, ColorJitter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "import random\n",
    "import utils\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130d6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c26c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveheavy(df, name, n):  #functions to save havy dataframes in multiple files\n",
    "    l= len(df)\n",
    "    for i in range(n-1):\n",
    "        df.iloc[int((l/n)*i):int((l/n)*(i+1))].to_hdf(f'{name}_{i+1}.h5', 'x', mode='w')\n",
    "    df.iloc[int((l/n)*(n-1)):l].to_hdf(f'{name}_{n}.h5', 'x', mode='w')\n",
    "    \n",
    "def readheavy(name, n, column, Dir):\n",
    "    result = pd.DataFrame(columns=column)\n",
    "    for i in range(n):\n",
    "        df = pd.read_hdf(f'Data/{Dir}/{name}_{i+1}.h5', 'x')\n",
    "        result = pd.concat([result, df], ignore_index=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddfbfa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restore Datasets\n",
    "\n",
    "test = readheavy('test', 2, ['audio', 'y'], 'Audio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb72a525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test['stft'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce61d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = readheavy('validation', 2, ['audio', 'y'], 'Audio')\n",
    "training = readheavy('training', 16, ['audio', 'y'], 'Audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8158f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stft(data):\n",
    "    df = pd.DataFrame(columns=['stft', 'y'])\n",
    "    for j in range(len(data)):\n",
    "        audio = data.loc[j, 'audio']\n",
    "        stft = np.abs(librosa.stft(audio, n_fft=1024, hop_length=512))\n",
    "        y = data.loc[j, 'y']\n",
    "        df = df.append({'stft': stft, 'y': y,}, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def get_mel(data):\n",
    "    df = pd.DataFrame(columns=['mel', 'y'])\n",
    "    for j in range(len(data)):\n",
    "        audio = data.loc[j, 'audio']\n",
    "        stft = np.abs(librosa.stft(audio, n_fft=1024, hop_length=512))\n",
    "        y = data.loc[j, 'y']\n",
    "        df = df.append({'mel': stft, 'y': y,}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d75a2a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I concert Audio into stft data\n",
    "\n",
    "test = get_stft(test)\n",
    "validation = get_stft(validation)\n",
    "training = get_stft(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdff50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_stft(df, n_samples):\n",
    "    df_clip = pd.DataFrame(columns=list(df.columns))\n",
    "    t = len(df[df.columns[0]][1][1])\n",
    "    for j in range(len(df)):\n",
    "        full = df.loc[j, df.columns[0]].transpose()\n",
    "        n=0\n",
    "        while (n<(len(full)-n_samples)):\n",
    "            clip = full[n: (n+n_samples)]\n",
    "            y = df.loc[j, 'y']\n",
    "            df_clip = df_clip.append({df.columns[0]: clip, 'y': y,}, ignore_index=True)\n",
    "            n+=int(n_samples/2)\n",
    "    return df_clip\n",
    "\n",
    "def clip_audio(df, n_samples):\n",
    "    df_clip = pd.DataFrame(columns=list(df.columns))\n",
    "    t = len(df[df.columns[0]][1])\n",
    "    for j in range(len(df)):\n",
    "        full = df.loc[j, df.columns[0]]\n",
    "        n=0\n",
    "        while (n<(len(full)-n_samples)):\n",
    "            clip = full[n: (n+n_samples)]\n",
    "            y = df.loc[j, 'y']\n",
    "            df_clip = df_clip.append({df.columns[0]: clip, 'y': y,}, ignore_index=True)\n",
    "            n+=int(n_samples/2)\n",
    "    return df_clip\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5734007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clip = clip(test, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15d0f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_clip = clip(validation, 128)\n",
    "training_clip = clip(training, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "912134f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121486,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_clip['stft'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0454bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#Class for the creation of torch manageble datasets, with Format one can select the desired input column \n",
    "class DataAudio(Dataset):\n",
    "\n",
    "    def __init__(self, split, Format, transform=None):\n",
    "        self.x = split[Format]\n",
    "        self.y = split['y']\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(32000, len(self.x))\n",
    "        #return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx, 0, 0].astype(float)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "081acbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 513)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clip['stft'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c6997f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(), #this converts numpy or Pil image to torch tensor and normalizes it in 0, 1\n",
    "    RandomAffine((0.05, 0.05)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76641069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of torch suited dataset classes  (change string 'mel' to select desired Format)\n",
    "training_dataset = DataAudio(training_clip, 'stft',transforms)\n",
    "validation_dataset = DataAudio(validation_clip, 'stft',transforms)\n",
    "test_dataset = DataAudio(test_clip, 'stft',transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad5d3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of dataloader classes\n",
    "batch_size = 64\n",
    "training_dataloader = DataLoader(training_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to add dropout layers \n",
    "\n",
    "class NNET1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNET1, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=128,kernel_size=(4,513)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(4,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(4,1)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Input of fc1 is 256\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 150),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(150, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        max_pool = F.max_pool2d(x, kernel_size=(26,1))\n",
    "        avg_pool = F.avg_pool2d(x, kernel_size=(26,1))\n",
    "        x = max_pool + avg_pool\n",
    "        x = self.fc(x.view(-1, 256))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbe510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNET2(nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(NNET2, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.c1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=256,kernel_size=(4,513)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.c2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(4, 1),padding=(2,0)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.c3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(4, 1),padding=(1,0)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 150),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(150, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        c1 = self.c1(x)\n",
    "        c2 = self.c2(c1)\n",
    "        c3 = self.c3(c2)\n",
    "        x = c1 + c3\n",
    "        max_pool = F.max_pool2d(x, kernel_size=(125,1))\n",
    "        avg_pool = F.avg_pool2d(x, kernel_size=(125,1))\n",
    "        x = max_pool + avg_pool\n",
    "        x = self.fc(x.view(-1, 256))\n",
    "        return x \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e98891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake spectrogram 128x513\n",
    "x = torch.randn(1, 1, 128, 513)\n",
    "\n",
    "# Create model\n",
    "model = NNET2()\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "# Print output shape\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
