{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 08:33:37.732862: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from utils_mix import NNET1D, LN1D, NNET2, LN2D\n",
    "import numpy as np\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os \n",
    "from torch.optim import Adadelta\n",
    "import pytorch_lightning as pl\n",
    "import pickle\n",
    "import utils\n",
    "from utils_mgr import DataAudio, create_subset, MinMaxScaler\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network initialized\n",
      "Using default optimizer parameters\n",
      "Network initialized\n",
      "Weights not initialised. If previous checkpoint is not loaded, set initialisation = \"xavier\"\n",
      "Using default optimizer parameters\n",
      "optimzier parameters: Adadelta (\n",
      "Parameter Group 0\n",
      "    differentiable: False\n",
      "    eps: 1e-06\n",
      "    foreach: None\n",
      "    lr: 1.0\n",
      "    maximize: False\n",
      "    rho: 0.9\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load model weights from checkpoint\n",
    "CKPT_PATH_1D = \"./1Dcheckpoint_Albi.ckpt\"\n",
    "CKPT_PATH_2D = \"../lightning_logs_2D_final/version_4/checkpoints/epoch=69-step=7000.ckpt\"\n",
    "nnet1d = LN1D.load_from_checkpoint(checkpoint_path=CKPT_PATH_1D).eval()\n",
    "nnet2d = LN2D.load_from_checkpoint(checkpoint_path=CKPT_PATH_2D).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the weights\n",
    "for param in nnet1d.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in nnet2d.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import warnings\n",
    "import librosa\n",
    "from utils_mgr import getAudio\n",
    "\n",
    "class DataAudio_double(Dataset):\n",
    "\n",
    "    def __init__(self, df, transform = None, type = \"1D\"):\n",
    "        \n",
    "        # Get track index\n",
    "        self.track_ids = df['index'].values\n",
    "\n",
    "        #Get genre label\n",
    "        self.label = df['labels'].values\n",
    "\n",
    "        #Transform\n",
    "        self.transform = transform\n",
    "\n",
    "        #Select type of input\n",
    "        self.type = type\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.track_ids)\n",
    "\n",
    "\n",
    "    def create_input(self, i):\n",
    "      \n",
    "        # Get audio\n",
    "\n",
    "        # load audio track\n",
    "        #with warnings.catch_warnings():\n",
    "        #    warnings.simplefilter('ignore')\n",
    "\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            audio, sr = getAudio(self.track_ids[i])\n",
    "\n",
    "            #Select random clip from audio\n",
    "            start = np.random.randint(0, (audio.shape[0]-2**18))\n",
    "            audio = audio[start:start+2**18]\n",
    "            \n",
    "        \n",
    "            #Get 2D spectrogram\n",
    "            stft = np.abs(librosa.stft(audio, n_fft=4096, hop_length=2048))\n",
    "            \n",
    "            mel = librosa.feature.melspectrogram(sr=sr, S=stft**2, n_mels=513)[:,:128]\n",
    "            mel = librosa.power_to_db(mel, ref=np.max).T\n",
    "    \n",
    "        \n",
    "            return audio[np.newaxis,:], mel\n",
    "        \n",
    "            \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # get input and label\n",
    "        try:\n",
    "            audio,mel = self.create_input(idx)\n",
    "            y = self.label[idx] \n",
    "        except:\n",
    "            print(\"\\nNot able to load track number \", self.track_ids[idx], \" Loading next one\\n\")\n",
    "            audio,mel = self.create_input(idx+1)\n",
    "            y = self.label[idx]\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            mel = self.transform(mel)\n",
    "           \n",
    "        return audio,mel,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# to visualize logs\n",
    "\n",
    "def import_and_preprocess_data(architecture_type=\"1D\"):\n",
    "\n",
    "    os.chdir(\"../.\")\n",
    "    files = os.listdir()\n",
    "    #print(files)\n",
    "    \"\"\"\n",
    "    This function uses metadata contained in tracks.csv to import mp3 files,\n",
    "    pass them through DataAudio class and eventually create Dataloaders.  \n",
    "    \n",
    "    \"\"\"\n",
    "    # Load metadata and features.\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "\n",
    "    #Select the desired subset among the entire dataset\n",
    "    sub = 'small'\n",
    "    raw_subset = tracks[tracks['set', 'subset'] <= sub] \n",
    "    \n",
    "    #Creation of clean subset for the generation of training, test and validation sets\n",
    "    meta_subset= create_subset(raw_subset)\n",
    "\n",
    "    # Remove corrupted files\n",
    "    corrupted = [98565, 98567, 98569, 99134, 108925, 133297]\n",
    "    meta_subset = meta_subset[~meta_subset['index'].isin(corrupted)]\n",
    "\n",
    "    #Split between taining, validation and test set according to original FMA split\n",
    "\n",
    "    train_set = meta_subset[meta_subset[\"split\"] == \"training\"]\n",
    "    val_set   = meta_subset[meta_subset[\"split\"] == \"validation\"]\n",
    "    test_set  = meta_subset[meta_subset[\"split\"] == \"test\"]\n",
    "\n",
    "    # Standard transformations for images\n",
    "\n",
    "    # There are two ways to normalize data: \n",
    "    #   1. Using  v2.Normalize(mean=[1.0784853], std=[4.0071154]). These values are computed with utils_mgr.mean_computer() function.\n",
    "    #   2. Using v2.Lambda and MinMaxScaler. This function is implemented in utils_mgr and resambles sklearn homonym function.\n",
    "\n",
    "    transforms = v2.Compose([v2.ToTensor(),\n",
    "        v2.RandomResizedCrop(size=(128,513), antialias=True), # Data Augmentation\n",
    "        v2.RandomHorizontalFlip(p=0.5), # Data Augmentation\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        #v2.Normalize(mean=[1.0784853], std=[4.0071154]),\n",
    "        v2.Lambda(lambda x: MinMaxScaler(x)) # see utils_mgr\n",
    "        ])\n",
    "\n",
    "    # Create the datasets and the dataloaders\n",
    "    \"\"\"  \n",
    "    train_dataset    = DataAudio_double(train_set, transform = transforms,type=architecture_type)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=os.cpu_count())\n",
    "    \"\"\"\n",
    "    val_dataset      = DataAudio_double(val_set, transform = transforms,type=architecture_type)\n",
    "    val_dataloader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=os.cpu_count())\n",
    "    \"\"\"\n",
    "    test_dataset     = DataAudio_double(test_set, transform = transforms,type=architecture_type)\n",
    "    test_dataloader  = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=os.cpu_count())\n",
    "    \"\"\"\n",
    "\n",
    "    #return train_dataloader, val_dataloader, test_dataloader\n",
    "    return val_dataloader, val_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['README.md', 'LitNet1D_Final.py', 'data', 'v2_2DLitNet.py', 'trialv2.pickle', 'music_genre_classification.ipynb', 'LitNet1D_prototype.py', 'trial.pickle', 'lightning_logs_2D_final', 'lightning_logs_first_trained_model', 'utils.py', '__pycache__', 'autoencoder_study', 'mix', 'appunti.txt', 'failed_GridseachSkorch.py', 'colab_tenatives', 'h5_experimentation', '.ipynb_checkpoints', 'optunav2.py', 'Old', 'utils_mgr.py', '.git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/miniconda3/envs/fma/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/diego/miniconda3/envs/fma/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/diego/miniconda3/envs/fma/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = import_and_preprocess_data(architecture_type=\"2D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(\"batch[0]\",batch[0].shape)\n",
    "    print(\"batch[1]\",batch[1].shape)\n",
    "    print(\"batch[2]\",batch[2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 256, kernel_size=(4, 513), stride=(1, 1))\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(256, 256, kernel_size=(4, 1), stride=(1, 1), padding=(2, 0))\n",
      "  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): Conv2d(256, 256, kernel_size=(4, 1), stride=(1, 1), padding=(1, 0))\n",
      "  (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Get all convolutional layers from nnet2d\n",
    "conv_layers = [layer for layer in nnet2d.modules() if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.ReLU)]\n",
    "\n",
    "# Build a new convolutional layer\n",
    "conv_block2D = nn.Sequential(*conv_layers[:9]) # [:9] to remove redundat ReLU layers taken by mistake from fc layers\n",
    "\n",
    "# Print the new convolutional layer\n",
    "print(conv_block2D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv1d(1, 16, kernel_size=(128,), stride=(32,), padding=(64,))\n",
      "  (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Conv1d(16, 32, kernel_size=(32,), stride=(2,), padding=(16,))\n",
      "  (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): Conv1d(32, 64, kernel_size=(16,), stride=(2,), padding=(8,))\n",
      "  (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): Conv1d(64, 128, kernel_size=(8,), stride=(2,), padding=(4,))\n",
      "  (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (11): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Get all convolutional layers from nnet1d\n",
    "conv_layers = [layer for layer in nnet1d.modules() if isinstance(layer, nn.Conv1d) or isinstance(layer, nn.BatchNorm1d) or isinstance(layer, nn.ReLU)]\n",
    "\n",
    "# Build a new convolutional layer\n",
    "conv_block1D = nn.Sequential(*conv_layers[:12])\n",
    "\n",
    "# Print the new convolutional layer\n",
    "print(conv_block1D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the weights are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print weights of the first conv layer in nnet1d\n",
    "for module in nnet2d.modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        print(module.weight[0])\n",
    "     \n",
    "        break\n",
    "\n",
    "# Print weights of the first conv layer in conv_block2D\n",
    "for module in conv_block2D.modules():\n",
    "    \n",
    "    if  isinstance(module, torch.nn.Conv2d):\n",
    "        print(module.weight[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixNet(nn.Module):\n",
    "    def __init__(self, conv_block1D, conv_block2D):\n",
    "        super(MixNet, self).__init__()\n",
    "        self.conv_block1D = conv_block1D\n",
    "        self.conv_block2D = conv_block2D\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Add dropout layer\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512+2048, 512),\n",
    "            nn.ReLU(),\n",
    "            self.dropout, \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            self.dropout, \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            self.dropout,  \n",
    "            nn.Linear(128, 8),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        audio = x[0]\n",
    "        mel = x[1]\n",
    "        \n",
    "        conv2d = self.conv_block2D(mel)\n",
    "        max_pool = F.max_pool2d(conv2d, kernel_size=(125,1))\n",
    "        avg_pool = F.avg_pool2d(conv2d, kernel_size=(125,1))\n",
    "        cat2d = torch.cat([max_pool,avg_pool],dim=1)\n",
    "        cat2d = cat2d.view(cat2d.size(0), -1) # cat2d shape torch.Size([1, 512])\n",
    "        \n",
    "        conv1d = self.conv_block1D(audio)\n",
    "        max_pool = F.max_pool1d(conv1d, kernel_size=125)\n",
    "        avg_pool = F.avg_pool1d(conv1d, kernel_size=125)\n",
    "        cat1d = torch.cat([max_pool,avg_pool],dim=1)\n",
    "        cat1d = cat1d.view(cat1d.size(0), -1) # cat1d dim = torch.Size([batch_size, 2048])\n",
    "\n",
    "        # Concatanate the two outputs and pass it to the classifier\n",
    "        # cat1d dim = torch.Size([batch_size, 2048])\n",
    "        # cat2d dim = torch.Size([batch_size, 512])\n",
    "        x = torch.cat([cat1d, cat2d], dim=1) \n",
    "        x = self.dropout(x)  # Add dropout layer\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = MixNet(conv_block1D, conv_block2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixNet(\n",
       "  (conv_block1D): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(128,), stride=(32,), padding=(64,))\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(16, 32, kernel_size=(32,), stride=(2,), padding=(16,))\n",
       "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv1d(32, 64, kernel_size=(16,), stride=(2,), padding=(8,))\n",
       "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv1d(64, 128, kernel_size=(8,), stride=(2,), padding=(4,))\n",
       "    (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_block2D): Sequential(\n",
       "    (0): Conv2d(1, 256, kernel_size=(4, 513), stride=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(256, 256, kernel_size=(4, 1), stride=(1, 1), padding=(2, 0))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(256, 256, kernel_size=(4, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2560, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): Linear(in_features=128, out_features=8, bias=True)\n",
       "    (10): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 262144])\n",
      "torch.Size([64, 8])\n",
      "tensor([[0.1213, 0.1137, 0.1288, 0.1365, 0.1382, 0.1308, 0.1224, 0.1084],\n",
      "        [0.1244, 0.1112, 0.1438, 0.1309, 0.1199, 0.1230, 0.1260, 0.1207],\n",
      "        [0.1199, 0.1165, 0.1318, 0.1404, 0.1101, 0.1317, 0.1315, 0.1183],\n",
      "        [0.1430, 0.1123, 0.1437, 0.1359, 0.1118, 0.1118, 0.1172, 0.1242],\n",
      "        [0.1221, 0.1146, 0.1404, 0.1298, 0.1195, 0.1277, 0.1291, 0.1168],\n",
      "        [0.1310, 0.1046, 0.1429, 0.1287, 0.1171, 0.1187, 0.1288, 0.1282],\n",
      "        [0.1347, 0.1065, 0.1394, 0.1487, 0.1120, 0.1122, 0.1211, 0.1253],\n",
      "        [0.1395, 0.1180, 0.1195, 0.1321, 0.1307, 0.1279, 0.1319, 0.1003],\n",
      "        [0.1242, 0.1009, 0.1432, 0.1438, 0.1223, 0.1337, 0.1226, 0.1093],\n",
      "        [0.1253, 0.1161, 0.1438, 0.1260, 0.1129, 0.1331, 0.1277, 0.1151],\n",
      "        [0.1347, 0.1143, 0.1240, 0.1296, 0.1213, 0.1294, 0.1301, 0.1166],\n",
      "        [0.1369, 0.1113, 0.1465, 0.1307, 0.1194, 0.1198, 0.1176, 0.1177],\n",
      "        [0.1261, 0.1135, 0.1319, 0.1320, 0.1320, 0.1220, 0.1276, 0.1147],\n",
      "        [0.1358, 0.1109, 0.1354, 0.1339, 0.1165, 0.1298, 0.1149, 0.1228],\n",
      "        [0.1337, 0.1167, 0.1408, 0.1278, 0.1167, 0.1225, 0.1276, 0.1143],\n",
      "        [0.1304, 0.1128, 0.1460, 0.1417, 0.1042, 0.1192, 0.1249, 0.1209],\n",
      "        [0.1232, 0.1146, 0.1232, 0.1326, 0.1324, 0.1275, 0.1258, 0.1207],\n",
      "        [0.1131, 0.1178, 0.1287, 0.1436, 0.1325, 0.1334, 0.1158, 0.1150],\n",
      "        [0.1327, 0.1266, 0.1270, 0.1239, 0.1349, 0.1153, 0.1287, 0.1109],\n",
      "        [0.1212, 0.1030, 0.1429, 0.1370, 0.1233, 0.1276, 0.1262, 0.1189],\n",
      "        [0.1409, 0.1283, 0.1270, 0.1296, 0.1150, 0.1174, 0.1213, 0.1205],\n",
      "        [0.1190, 0.1094, 0.1302, 0.1509, 0.1227, 0.1305, 0.1283, 0.1090],\n",
      "        [0.1414, 0.1159, 0.1325, 0.1368, 0.1251, 0.1225, 0.1185, 0.1072],\n",
      "        [0.1208, 0.1024, 0.1295, 0.1439, 0.1210, 0.1405, 0.1281, 0.1138],\n",
      "        [0.1128, 0.1218, 0.1403, 0.1396, 0.1188, 0.1238, 0.1286, 0.1143],\n",
      "        [0.1366, 0.1129, 0.1310, 0.1335, 0.1185, 0.1311, 0.1210, 0.1154],\n",
      "        [0.1292, 0.1048, 0.1374, 0.1460, 0.1247, 0.1220, 0.1214, 0.1143],\n",
      "        [0.1331, 0.1174, 0.1374, 0.1416, 0.1167, 0.1201, 0.1217, 0.1121],\n",
      "        [0.1313, 0.1036, 0.1384, 0.1352, 0.1261, 0.1240, 0.1212, 0.1203],\n",
      "        [0.1263, 0.1115, 0.1448, 0.1321, 0.1283, 0.1339, 0.1120, 0.1112],\n",
      "        [0.1353, 0.1245, 0.1289, 0.1310, 0.1231, 0.1235, 0.1191, 0.1145],\n",
      "        [0.1328, 0.1170, 0.1435, 0.1331, 0.1130, 0.1162, 0.1279, 0.1165],\n",
      "        [0.1168, 0.1253, 0.1233, 0.1399, 0.1047, 0.1470, 0.1369, 0.1060],\n",
      "        [0.1346, 0.1119, 0.1297, 0.1390, 0.1224, 0.1308, 0.1214, 0.1101],\n",
      "        [0.1315, 0.1057, 0.1417, 0.1212, 0.1371, 0.1146, 0.1208, 0.1274],\n",
      "        [0.1278, 0.1237, 0.1370, 0.1294, 0.1211, 0.1191, 0.1203, 0.1217],\n",
      "        [0.1257, 0.1267, 0.1346, 0.1458, 0.1079, 0.1233, 0.1247, 0.1112],\n",
      "        [0.1316, 0.1096, 0.1314, 0.1310, 0.1334, 0.1139, 0.1264, 0.1226],\n",
      "        [0.1277, 0.1216, 0.1371, 0.1235, 0.1191, 0.1285, 0.1246, 0.1179],\n",
      "        [0.1279, 0.1152, 0.1372, 0.1355, 0.1297, 0.1180, 0.1282, 0.1083],\n",
      "        [0.1272, 0.1164, 0.1192, 0.1527, 0.1228, 0.1348, 0.1208, 0.1061],\n",
      "        [0.1314, 0.1035, 0.1334, 0.1293, 0.1245, 0.1392, 0.1350, 0.1036],\n",
      "        [0.1347, 0.1058, 0.1381, 0.1230, 0.1143, 0.1407, 0.1273, 0.1162],\n",
      "        [0.1368, 0.1136, 0.1332, 0.1286, 0.1223, 0.1239, 0.1186, 0.1230],\n",
      "        [0.1223, 0.1182, 0.1319, 0.1300, 0.1223, 0.1280, 0.1267, 0.1204],\n",
      "        [0.1202, 0.1001, 0.1555, 0.1455, 0.1230, 0.1179, 0.1198, 0.1181],\n",
      "        [0.1325, 0.1132, 0.1278, 0.1373, 0.1307, 0.1244, 0.1216, 0.1125],\n",
      "        [0.1203, 0.1247, 0.1223, 0.1358, 0.1242, 0.1307, 0.1293, 0.1127],\n",
      "        [0.1207, 0.1081, 0.1282, 0.1450, 0.1206, 0.1384, 0.1226, 0.1164],\n",
      "        [0.1381, 0.1130, 0.1370, 0.1293, 0.1236, 0.1215, 0.1217, 0.1159],\n",
      "        [0.1238, 0.1108, 0.1386, 0.1363, 0.1196, 0.1259, 0.1324, 0.1125],\n",
      "        [0.1295, 0.1085, 0.1453, 0.1694, 0.1094, 0.1304, 0.1116, 0.0959],\n",
      "        [0.1235, 0.1100, 0.1347, 0.1475, 0.1118, 0.1407, 0.1313, 0.1005],\n",
      "        [0.1417, 0.1094, 0.1516, 0.1266, 0.1127, 0.1186, 0.1260, 0.1136],\n",
      "        [0.1152, 0.1017, 0.1436, 0.1359, 0.1191, 0.1427, 0.1260, 0.1157],\n",
      "        [0.1412, 0.1063, 0.1276, 0.1388, 0.1378, 0.1195, 0.1164, 0.1124],\n",
      "        [0.1312, 0.1093, 0.1564, 0.1157, 0.1256, 0.1228, 0.1266, 0.1124],\n",
      "        [0.1250, 0.1209, 0.1329, 0.1389, 0.1099, 0.1288, 0.1255, 0.1180],\n",
      "        [0.1376, 0.1035, 0.1313, 0.1380, 0.1163, 0.1172, 0.1324, 0.1236],\n",
      "        [0.1263, 0.1130, 0.1379, 0.1304, 0.1233, 0.1294, 0.1218, 0.1179],\n",
      "        [0.1196, 0.1135, 0.1341, 0.1509, 0.1165, 0.1310, 0.1235, 0.1108],\n",
      "        [0.1235, 0.1000, 0.1564, 0.1312, 0.1182, 0.1394, 0.1139, 0.1174],\n",
      "        [0.1414, 0.1146, 0.1464, 0.1282, 0.1173, 0.1318, 0.1155, 0.1049],\n",
      "        [0.1255, 0.1155, 0.1289, 0.1319, 0.1298, 0.1310, 0.1260, 0.1113]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    input = batch[:2]\n",
    "    print(input[0].shape) # Audio shape: torch.Size([64, 1, 262144])\n",
    "    label = batch[2]\n",
    "    print(label.shape) # Label shape: torch.Size([64, 8])\n",
    "    output = mn(input)\n",
    "    print(output) \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only classifier layers need grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_block1D.0.weight: False\n",
      "conv_block1D.0.bias: False\n",
      "conv_block1D.1.weight: False\n",
      "conv_block1D.1.bias: False\n",
      "conv_block1D.3.weight: False\n",
      "conv_block1D.3.bias: False\n",
      "conv_block1D.4.weight: False\n",
      "conv_block1D.4.bias: False\n",
      "conv_block1D.6.weight: False\n",
      "conv_block1D.6.bias: False\n",
      "conv_block1D.7.weight: False\n",
      "conv_block1D.7.bias: False\n",
      "conv_block1D.9.weight: False\n",
      "conv_block1D.9.bias: False\n",
      "conv_block1D.10.weight: False\n",
      "conv_block1D.10.bias: False\n",
      "conv_block2D.0.weight: False\n",
      "conv_block2D.0.bias: False\n",
      "conv_block2D.1.weight: False\n",
      "conv_block2D.1.bias: False\n",
      "conv_block2D.3.weight: False\n",
      "conv_block2D.3.bias: False\n",
      "conv_block2D.4.weight: False\n",
      "conv_block2D.4.bias: False\n",
      "conv_block2D.6.weight: False\n",
      "conv_block2D.6.bias: False\n",
      "conv_block2D.7.weight: False\n",
      "conv_block2D.7.bias: False\n",
      "classifier.0.weight: True\n",
      "classifier.0.bias: True\n",
      "classifier.3.weight: True\n",
      "classifier.3.bias: True\n",
      "classifier.6.weight: True\n",
      "classifier.6.bias: True\n",
      "classifier.9.weight: True\n",
      "classifier.9.bias: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in mn.named_parameters():\n",
    "    print(f'{name}: {param.requires_grad}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
